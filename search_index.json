[
["index.html", "Chapter One - A Journey with DataKind SG Welcome Goals", " Chapter One - A Journey with DataKind SG DataKind SG 2017-10-10 Welcome Welcome to a new chapter in your journey with us at DataKind-SG! As with all great stories, we start with chapter one… At DataKind Singapore, we have worked with several Social Sector Partners to deliver data projects that aim to derive insights from their data and produce analyses and data products. We have since organized multiple Meetups to work on these data projects. This is a book on the lessons learnt and best practices derived from these data projects on executing large collaborative data projects. NOTE: This book is still in Alpha with many sections to be fleshed out. We seek your understanding should you encounter these sections. Nevertheless we still hope you have learnt a few tips from here. Goals Understand NGO’s requirements fully and translate requirements into clear specific tasks Quick view and easy management of project progress Facilitate sharing and mass collaboration Reproducible pipeline, from environment to code, data analyses, documentation and products Requirements traceability Meant to be a guide rather than a dogmatic approach for future DataKind Data Ambassadorss, Project Managers and Data Experts as well as anyone involved in data projects "],
["overview-of-project-flow.html", "Chapter 1 Overview of Project Flow 1.1 Project Funnel 1.2 Project Accelerators 1.3 DataJams 1.4 DataDive 1.5 DataCorps 1.6 DataLearn", " Chapter 1 Overview of Project Flow We shall have an overview of the overall data project flow in this chapter. This helps to set the context and helps you relate to the recommended practices described in this book. Subsequently we shall be taking a deep dive into the specific topics. While each of these chapters can be read independently depending on your needs, it references the project flow heavily and it helps if you read this chapter first. 1.1 Project Funnel In the picture below, you will see the project funnel that DataKind SG uses to assist our social sector partners DataKind Project Funnel 1.2 Project Accelerators This is the session where we bring the data science community together with our social sector partners in brainstorming sessions to take the first steps towards a successful data science project. 1.3 DataJams Once data has been obtained, we hold DataJam sessions that bring the data experts together with social sector partners to clean up and prepare the data prior to DataDives and/or DataCorps 1.4 DataDive DataDives are where the data experts and social sector partners collaborate to generate data solutions and/or products, tackling tough problems over the course of a weekend. 1.5 DataCorps DataCorps are specialized teams of pro bono data scientists who work on indepth, 6 to 9-month projects for social sector partners. 1.6 DataLearn From time-to-time, we may also conduct DataLearn sessions, where we help prepare our volunteers for DataDives through hands-on tutorials and workshops. "],
["project-management.html", "Chapter 2 Project Management 2.1 Tracking Progress 2.2 Tasks and Assignment 2.3 Recommended Roles 2.4 Tips", " Chapter 2 Project Management 2.1 Tracking Progress We use Trello for tracking progress. 2.2 Tasks and Assignment 2.3 Recommended Roles 2.4 Tips "],
["data-cleaning.html", "Chapter 3 Data Cleaning 3.1 Test Driven Data Cleaning", " Chapter 3 Data Cleaning 3.1 Test Driven Data Cleaning 3.1.1 For Volunteers Test Driven Data Cleaning is an approach that incorporates principles of Test Driven Development to achieve collaborative and reproducible data cleaning. Basically, data cleaning scripts are divided into test scripts and cleaning scripts. Test scripts document the expected outcomes from the data cleaning, hence we should aim to create the tests first before cleaning. Within each file you will see functions corresponding to the columns in the dataset pre-populated for you. The file names of test scripts are in the format test_clean_&lt;file name of dataset&gt;. Cleaning scripts are where you write code to the dataset. Similar to the test scripts, within cleaning script file, you will also see functions corresponding to the columns in the dataset pre-populated for you. On execution, the cleaning script will iterate through each column row-by-row to clean the dataset. The file names of cleaning scripts are in the format clean_&lt;file name of dataset&gt;. 3.1.1.1 Cloning the repository For version control, we are using Github. To clone (or get a copy of) the repository, run $ git clone &lt;path to Github repository&gt; 3.1.1.2 Assigning yourself a column To track progress, we use Trello boards. There will be a dedicated Trello board for each dataset file. Within the Trello board, you will be able to see a list of backlogs, where each Trello card corresponds to the name of a column, as shown in the below image. If you open each card, you will be able to see summary statistics that can be useful for writing methods to clean the column. Cleaning Backlog To start working on, say, column 1, add yourself to the corresponding Trello card. Subsequently, proceed to create a git branch by entering the following command. Creating the git branch allows you to make changes in isolation without affecting the master code base. We recommend the branch name to be in the form of test_clean_col_&lt;number&gt;. $ git checkout -b test_clean_col_1 3.1.1.3 Testing and cleaning a column for Python users In the test_clean_&lt;file name of dataset&gt;.py file, modify the test method as necessary. An example of the test method is shown below. @pytest.mark.skip def test_clean_col_1(): &quot;&quot;&quot; Test the cleaning for column: &quot;date&quot; &quot;&quot;&quot; assert clean_sample_data.clean_col_1(&#39;NA&#39;) == &#39;NA&#39; assert False By default, the test for column 1 is skipped. To enable the test, you will have to comment or remove the line @pytest.mark.skip as well as the line assert False. Subsequently, you can proceed to define the test(s) by using the assert statement. To test, run $ pytest test_clean_&lt;file name of dataset&gt;.py You will see the test results similar to the below: ============================= test session starts ============================== platform linux -- Python 3.6.2, pytest-3.2.1, py-1.4.34, pluggy-0.4.0 rootdir: /path/to/directory, inifile: collected 3 items test_clean_sample_data.py s.F =================================== FAILURES =================================== _______________________________ test_clean_col_2 _______________________________ def test_clean_col_2(): &quot;&quot;&quot; Test the cleaning for column: &quot;country&quot; &quot;&quot;&quot; &gt; assert clean_sample_data.clean_col_2(&#39;&#39;) == &#39;NA&#39; # expects test to fail E AssertionError: assert &#39;some wrong value&#39; == &#39;NA&#39; E - some wrong value E + NA test_clean_sample_data.py:28: AssertionError ================ 1 failed, 1 passed, 1 skipped in 0.03 seconds ================= In this example, you will see that there is 1 test function for each test outcome (failed, passed or skipped). You can also see a short version of the results in the line test_clean_sample_data.py s.F where ‘s’ means ‘skipped’, ‘.’ means ‘passed’, and ‘F’ means ‘failed’. You will also see which test has failed and its actual and expected output. Please note that you may have to install the pytest package to ensure the above command can be executed. To install pytest you can do so with one of the following commands: $ pip3 install pytest # install with pip (Python 3) $ conda install pytest # OR install with conda To generate the cleaned dataset, run $ python clean_&lt;file name of dataset&gt;.py You can find the generated file that is named cleaned_&lt;file name of dataset&gt;.csv in the designated directory. You can download and run the completed example test and cleaning scripts in the links below. test_clean_sample_data.py clean_sample_data.py sample_data.csv 3.1.1.4 Commit changes and create pull requests Run the following command to push your changes to Github. $ git add . $ git commit -m &quot;&lt;brief description of changes&gt;&quot; $ git push origin test_clean_col_1 Subsequently, login to Github and submit a pull request for the branch that you have just changed. Reference here. 3.1.2 For Team Leads 3.1.2.1 Pre-requisites You will need to have a Trello account. You can sign up for one at https://trello.com You will also need to install the tddc package. To get the latest version from Github, run the following: $ pip3 install git+https://github.com/DataKind-SG/test-driven-data-cleaning.git Last but not least, your dataset needs to be in CSV format. 3.1.2.2 Creating the scaffolds and Trello board In this walkthrough we shall use an example CSV file that can be downloaded from the link below. sample_data.csv In the same directory as the file, run: $ tddc summarize sample_data.csv This takes the csv data set and summarizes it, outputing to a json file in a newly created output/ directory. If this is the first time you’re running this, you should create a Trello configuration file named .tddc_config.yml in your user root directory with the format: trello: api_key: &lt;TRELL_API_KEY&gt; token: &lt;TRELLO_TOKEN&gt; You can get your Trello API key here: https://trello.com/app-key Replace your Trello API key at the end of this URL to get your Trello token (set to expire in 1 day): https://trello.com/1/authorize?expiration=1day&amp;scope=read,write,account&amp;response_type=token&amp;name=Server%20Token&amp;key=&lt;TRELLO_API_KEY&gt; Next, you can run: $ tddc build_trello sample_data.csv If .tddc_config.yml has not yet been created, the command will fail and give you instructions on how to create a Trello configuration file in your root directory. Once you create it, you can try to run that step again. This will create a Trello board named ‘Data Cleaning board for: ’ under the ‘Personal Boards’ section. You can then optionally move the board to your team in Trello via Show Menu &gt; More &gt; Settings &gt; Change Team... Next, you can run: $ tddc build sample_data.csv This outputs a script into the output/ folder that contains method stubs and glue code to clean the data set. It also outputs stubs for tests in output/. Please refer to the previous section for volunteers to see how these test and cleaning scripts can be used. Finally, commit the test and cleaning scripts to your team’s Github repository. "],
["managing-code.html", "Chapter 4 Managing Code 4.1 Version Control 4.2 Github FAQs 4.3 Directory Structure 4.4 Code Style 4.5 Preamble information to be placed at the top of every script 4.6 Quickstart Templates 4.7 Tips", " Chapter 4 Managing Code 4.1 Version Control Version Control is important to track changes made and to enable collaboration with other volunteers during the project. For this, we shall be using Github. Please refer to the below sections for the most common questions our volunteers have on Github usage. 4.2 Github FAQs 4.2.1 The csv.gz files seems to be too small/corrupted, what should I do? You will need to install Git LFS. Instructions can be found in the link below: https://help.github.com/articles/installing-git-large-file-storage/ 4.2.2 I’m new to Git, where can I get more information? You can refer to the the resources section of this documentation. 4.2.3 How to clone a git repository? Go to the command line and type the following git clone https://github.com/DataKind-SG/&lt;project_name&gt;.git OR if you are using git large file storage: git-lfs clone https://github.com/DataKind-SG/&lt;project_name&gt;.git 4.2.4 How to load csv.gz files? If you are using R: read.csv(gzfile(&quot;filename.csv.gz&quot;)) OR library(readr) read_csv(&quot;filename.csv.gz&quot;) If you are using Python: import pandas as pd pd.read_csv(&#39;filename.csv.gz&#39;) 4.2.5 How can I safely edit the code and data files? We recommend that you first create a “branch” in git - think of it as a personal copy - and make your changes. Then you submit a “pull request” for review - think of it as a notification for the admins to review and update the main files 4.2.6 Doing a git branch We recommend to name your branches in the following format: “-” git checkout -b &quot;name of the branch&quot; 4.2.7 How to submit changes to the code repository git add . git commit -m &quot;brief description of change&quot; git push origin &lt;branch name&gt;&quot; To do a pull request, please refer to this link: https://help.github.com/articles/creating-a-pull-request/ 4.3 Directory Structure ├── _scripts | ├── subtask1 | ├── subtask2 ├── _data | ├── subdata1 | ├── subdata2 |--- _outputs 4.4 Code Style 4 spaces, not tabs 4.5 Preamble information to be placed at the top of every script Intent/Purpose What’s the purpose of the script Input Param/external data that the script needs Output What it produces Dependencies Libraries/setup that the script needs in order to run Sample Usage Sample demo on how to run/call the script You can view a sample Script here. To add the files into Docker, you need to additional environment information (quay.io docker filepath) e.g., for R: #‘docker_filepath: quay.io/dksg/children-society-r-notebook:x.x.x) - Author: e.g., for R: #’ author:Paul - Summary of the objective of script. e.g., #’ desc: takes in the file_1, file_2 and file_3, performs xxx, yyy, and zzz, and produces output_1 and output_2 4.6 Quickstart Templates dksg_template.R 4.7 Tips "],
["reproducible-environment-and-quality-assurance.html", "Chapter 5 Reproducible Environment and Quality Assurance 5.1 Using Containers, Continuous Integration and Delivery 5.2 Installation 5.3 Ensure that Docker is Running 5.4 Using Docker for Python Notebooks 5.5 Using Docker for R Notebooks 5.6 Using Docker for RStudio 5.7 Managing libraries 5.8 Testing 5.9 Recommended Roles 5.10 Tips", " Chapter 5 Reproducible Environment and Quality Assurance 5.1 Using Containers, Continuous Integration and Delivery To ensure that the scripts can run in a consistent environment, we create Docker containers with known versions of Development Environment and the corresponding library versions used in generating the data analysis/products. We will also need to keep the containers up to date and easily available for download. Hence we utilize Continuous Integration and Delivery capabilities such as Quay.io to build images from specified Dockerfiles. In this section, we shall walk you through the setup and running of different Docker environment. 5.2 Installation 5.2.1 … for Windows Follow the setup instructions here: https://docs.docker.com/docker-for-windows/install/ Note: If your machine doesn’t met the requirement for “Docker For Windows”, try setting up “Docker Toolbox”: https://docs.docker.com/toolbox/toolbox_install_windows/ 5.2.2 … for Linux Follow the setup instructions for your flavor of Linux here: https://docs.docker.com/engine/installation/linux/ 5.2.3 … for MacOS Follow the setup instructions here: https://store.docker.com/editions/community/docker-ce-desktop-mac Or if you use Homebrew Cask, $ brew cask install docker 5.3 Ensure that Docker is Running Start running the Docker app. Check that it is running on the command line: $ docker info Containers: 3 Running: 0 Paused: 0 Stopped: 3 Images: 1 Server Version: 1.13.1 ... 5.4 Using Docker for Python Notebooks 5.4.1 Getting a Python Jupyter Notebook Container Image There are at least two ways of getting an image: Pulling from a repository (such as quay.io) Loading from a file 5.4.2 Pulling from a repository You can pull down the image with: $ docker pull quay.io/dksg/python3-notebook:1.0.0 Once that finishes downloading, you should see something like: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/dksg/python3-notebook 1.0.0 f01e49a5a922 3 days ago 2.61 GB 5.4.3 Loading from a file This is an alternative method. Skip this if you already have pulled from a repository successfully. Otherwise, follow the steps below: Copy the tar file (get this from a DK corelead) to your local directory (e.g. quay.io_SLASH_dksg_SLASH_python3-notebook_1.0.0.tar) In your local directory, run the following docker command: docker load --input quay.io_SLASH_dksg_SLASH_python3-notebook_&lt;tagged_version&gt;.tar This will return a loaded image id. Tag the newly added image with the version from the filename by running the following: docker tag &lt;loaded image id&gt; quay.io/dksg/python3-notebook:&lt;tagged_version&gt; 5.4.4 Running a Jupyter Notebook from the pulled/loaded image Take the IMAGE ID from previous step and start it up with this command: docker run -p 8888:8888 -v /path/to/local/directory:/home/jovyan/work f01e49a5a922 Note: /path/to/local/directory should be replaced by an existing local directory in your laptop. This is where your notebooks (.ipynb) will be stored. e.g. docker run -p 8888:8888 -v /Users/johndoe/datadive:/home/jovyan/work quay.io/dksg/python3-notebook:1.0.0 You will get instructions for link to paste into your browser address box. If you’re using Docker Toolbox, you should use the custom IP address (default http://192.168.99.100/) Once the notebook is running, you may create a new notebook and try the samples in this tutorial: https://plot.ly/python/ipython-notebook-tutorial/ Note: The following python script may be needed to run first in order to run the above tutorial samples: import plotly plotly.offline.init_notebook_mode() # run at the start of every ipython notebook 5.5 Using Docker for R Notebooks 5.5.1 Getting an R Jupyter Notebook Container Image There are at least two ways of getting an image: Pulling from a repository (such as quay.io) Loading from a file 5.5.1.1 Pulling from a repository You can pull down the image with: $ docker pull quay.io/dksg/r-notebook:&lt;tagged_version&gt; Once that finishes downloading, you should see something like: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/dksg/r-notebook 1.0.1 f01e49a5a922 3 days ago 2.61 GB 5.5.1.2 Loading from a file This is an alternative method. Skip this if you already have pulled from a repository successfully. Otherwise, follow the steps below: Copy the tar file (get this from a DK corelead) to your local directory (e.g. quay.io_SLASH_dksg_SLASH_r-notebook_1.0.1.tar) In your local directory, run the following docker command: docker load --input quay.io_SLASH_dksg_SLASH_r-notebook_&lt;tagged_version&gt;.tar This will return a loaded image id. Tag the newly added image with the version from the filename by running the following: docker tag &lt;loaded image id&gt; quay.io/dksg/r-notebook:&lt;tagged_version&gt; 5.5.2 Running a Jupyter Notebook from the pulled/loaded image Take the IMAGE ID from previous step and start it up with this command: docker run -p 8888:8888 -v /path/to/local/directory:/home/jovyan/work f01e49a5a922 Note: /path/to/local/directory should be replaced by an existing local directory in your laptop. This is where your notebooks (.ipynb) will be stored. e.g. docker run -p 8888:8888 -v /Users/johndoe/datadive:/home/jovyan/work quay.io/dksg/r-notebook: You will get instructions for link to paste into your browser address box. If you’re using Docker Toolbox, you should use the custom IP address (default http://192.168.99.100/) 5.5.3 Once the notebook is running, you may create a new notebook and try the following samples: https://plot.ly/r/using-r-in-jupyter-notebooks/#examples 5.6 Using Docker for RStudio 5.6.1 Getting an RStudio Container Image There are at least two ways of getting an image: Pulling from a repository (such as quay.io) Loading from a file 5.6.1.1 Pulling from a repository You can pull down the image with: $ docker pull quay.io/dksg/&lt;project_name&gt;-rstudio:&lt;tagged_version&gt; Once that finishes downloading, you should see something like: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/dksg/&lt;project_name&gt;-rstudio 1.0.2 1c1e06209032 13 hours ago 1.166 GB 5.6.1.2 Loading from a file This is an alternative method. Skip this if you already have pulled from a repository successfully. Otherwise, follow the steps below: Copy the tar file (get this from a DK corelead) to your local directory (e.g. quay.io_SLASH_dksg_SLASH_ojoy-rstudio_1.0.2.tar) In your local directory, run the following docker command: docker load --input quay.io_SLASH_dksg_SLASH_&lt;project_name&gt;-rstudio_&lt;tagged_version&gt;.tar Once loaded, you should be able to see the new image when you run “docker images”: $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE quay.io/dksg/&lt;project_name&gt;-rstudio 1.0.2 1c1e06209032 13 hours ago 1.166 GB 5.6.2 Running RStudio from the pulled/loaded image Start it up with this command: docker run -p 8787:8787 -v /path/to/local/directory:/home/rstudio/foobar quay.io/dksg/&lt;project_name&gt;-rstudio:&lt;tagged_version&gt; Note: /path/to/local/directory should be replaced by an existing local directory in your laptop. This is where your data/scripts will be stored. e.g. docker run -d -p 8787:8787 -v /Users/johndoe/datadive:/home/rstudio/foobar quay.io/dksg/-rstudio: You should be able to access RStudio in the browser via http://localhost:8787. If you’re using Docker Toolbox, you should use the custom IP address (default http://192.168.99.100:8787) Username: rstudio Password: rstudio 5.7 Managing libraries If there’s a python or R library that you need, you can install it in your container, but unless the library is persisted to the image, your scripts that use the library will not run on somebody else’s machine. Each project will have a person assigned as a library curator and they will be able to include the library in the project’s docker image. Workflow should be: You’re puttering along when you realise that you want to add your favourite nlp library. You install it in your container, and try it out. It works great! Show it to your project’s curator and convince them that it’s a useful library. Their default mode is lazy and they will try to point you to an existing library. You show them the hot shiny feature the one you want has. The curator changes the requirements file in our docker file Github repo, Quay auto-magically builds a new image, and when people need to run your code, they need to use this new image. 5.8 Testing 5.9 Recommended Roles 5.10 Tips "],
["data-protection-and-information-ethics.html", "Chapter 6 Data Protection and Information Ethics 6.1 Data Protection 6.2 Information Ethics", " Chapter 6 Data Protection and Information Ethics We appeal to your common sense and sense of Responsibility here. 6.1 Data Protection Data Protection refers to the “control over access to and use of data stored in computers” As a volunteer, you are responsible to hold yourself to the highest standards of data collection, privacy and security. 6.2 Information Ethics Information Ethics refers to a “branch of ethics that focuses on the relationship between the creation, organization, dissemination, and use of information” As a volunteer, you are morally obliged to perform analysis that is ethically correct according to the guidelines set by your social sector partner. "],
["resources.html", "Chapter 7 Resources 7.1 Tools 7.2 Cheatsheets 7.3 Videos", " Chapter 7 Resources 7.1 Tools 7.1.1 Test Driven Data Cleaning Please refer to your respective project repository’s readme on how test driven data cleaning can be used. 7.1.1.1 Python users Github Repository 7.1.1.2 R users testthat on CRAN Getting started with testing 7.1.2 Github 7.1.2.1 Github Desktop and other IDEs Github Desktop User Guides Using version control with RStudio 7.1.2.2 Command Line git - the simple guide 7.1.3 Trello Trello Tutorial Getting Started with Trello (Video Demo) 7.2 Cheatsheets You can refer to the below links for cheatsheets that might be useful. 7.2.1 R Collection of RStudio Cheatsheets 7.2.2 Python Pandas Cheatsheet Python Cheatsheets 7.3 Videos DataLearn: Docker for Reproducible Research, presented by Paul, recorded by Engineers.SG "],
["about.html", "Chapter 8 About 8.1 About DKSG 8.2 Contributors", " Chapter 8 About 8.1 About DKSG About DataKind Singapore 8.2 Contributors "],
["references.html", "References", " References "]
]
